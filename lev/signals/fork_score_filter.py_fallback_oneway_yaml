#!/usr/bin/env python3
import os
import sys
import json
import logging
import redis
import yaml
import re
import argparse
import pandas as pd

# --- Dummy Redis for --no-redis mode or fallback ---
class DummyRedis:
    def get(self, *args, **kwargs):
        return None
    def sadd(self, *args, **kwargs):
        return 0
    def rpush(self, *args, **kwargs):
        return 0
    def delete(self, *args, **kwargs):
        return 0
from ta.momentum import stochrsi
from pathlib import Path
from datetime import datetime

# --- Leverage universe (10 coins) ---
ALLOWED_TICKERS = {"BTC","ETH","XRP","LTC","ADA","DOGE","ALGO","ETC","BCH","AXS"}
ALIASES = {"DODGE": "DOGE"}

def _to_usdt_symbol(s: str) -> str:
    s = (s or "").upper().strip()
    s = ALIASES.get(s, s)
    if s.endswith("USDT"):
        return s
    return f"{s}USDT"

# === Setup ===
CURRENT_FILE = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE.parent.parent
sys.path.append(str(PROJECT_ROOT))

# --- Load paths from config_loader ---
from config.config_loader import PATHS

# Default paths (can be overridden by CLI)
DEFAULT_CONFIG_PATH = Path("lev/signals/config/fork_config.yaml")
FORK_INPUT_FILE = PATHS["fork_candidates"]
DEFAULT_OUTPUT_FILE = PATHS["final_fork_rrr_trades"]
DEFAULT_BACKTEST_CANDIDATES_FILE = PATHS["fork_backtest_candidates"]
FORK_HISTORY_BASE = PATHS["fork_history"]
SNAPSHOT_BASE = PATHS["snapshots"]

# Redis keys are namespaced by mode (lev) and side (long/short)
REDIS_PREFIX = "LEV"
REDIS_SET = lambda side: f"{REDIS_PREFIX}_{side.upper()}_FORK_RRR_PASSED"
REDIS_FINAL_TRADES = lambda side: f"{REDIS_PREFIX}_{side.upper()}_FORK_FINAL_TRADES"

# Default to a no-op Redis; real client is created in main() unless --no-redis is set
r = DummyRedis()

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)

# === Config loader ===

def load_config(path: Path) -> tuple:
    if not Path(path).exists():
        logging.warning(f"Config not found at {path}; using built-in defaults for lev.")
        return (
            0.35,               # min_score default for lev
            {},                 # weights
            {"use_volume_penalty": True},
            {},
        )
    with open(path) as f:
        cfg = yaml.safe_load(f) or {}

    weights = (cfg.get("weights") or {}).copy()

    # Translate spot-style keys to lev scorer keys if detected
    # spot -> lev mapping
    key_map = {
        "macd_bearish_cross": "macd_trend_cross",
        "ema_price_reclaim": "ema_trend_ok",
        "stoch_overbought_penalty": "stoch_extreme_penalty",
        "rsi_recovery": "rsi_direction",
    }
    if any(k in weights for k in key_map.keys()):
        logging.warning("Detected spot-style weight keys in config; translating for lev scorer.")
        translated = {}
        for k, v in weights.items():
            nk = key_map.get(k, k)
            # Preserve sign; if original had negative penalty weights, they carry over
            translated[nk] = v
        weights = translated

    min_score = cfg.get("min_score", 0.35)
    options = cfg.get("options", {}) or {}
    return (min_score, weights, options, cfg)

# --- Module-level defaults (overridden in main) ---
MIN_SCORE = 0.73
WEIGHTS = {}
OPTIONS = {}


def extract_float(val):
    if val is None:
        return 0.0
    s = str(val).strip().replace("'", "").replace('"', "")
    if s.lower() == "none":
        return 0.0
    try:
        s_clean = s.replace("np.float64(", "").replace(")", "")
        return float(s_clean)
    except:
        match = re.search(r"[-+]?\d*\.\d+|\d+", s)
        return float(match.group()) if match else 0.0


def btc_sentiment_multiplier(side: str = "long"):
    price = extract_float(r.get("BTC_1h_latest_close")) if r else 0.0
    ema50 = extract_float(r.get("BTC_1h_EMA50")) if r else 0.0
    rsi = extract_float(r.get("BTC_15m_RSI14")) if r else 0.0
    adx = extract_float(r.get("BTC_1h_ADX14")) if r else 0.0
    mult = 1.0
    long_bias = (price > ema50)
    if side == "short":
        long_bias = not long_bias  # invert for short
    if long_bias and adx > 20:
        mult += 0.10
    elif long_bias:
        mult += 0.05
    elif (not long_bias) and adx > 15:
        mult -= 0.05
    if side == "long" and rsi < 35:
        mult -= 0.05
    if side == "short" and rsi > 65:
        mult -= 0.05
    return max(0.8, min(mult, 1.2))


def compute_stoch_slope(symbol):
    today = datetime.utcnow().strftime("%Y-%m-%d")
    filepath = SNAPSHOT_BASE / today / f"{symbol.upper()}_15m_klines.json"
    if not filepath.exists():
        return 0.0, None
    try:
        with open(filepath) as f:
            klines = json.load(f)
        closes = [float(k[4]) for k in klines][-30:]
        if len(closes) < 20:
            return 0.0, None
        df = pd.DataFrame({"close": closes})
        df["stoch_rsi_k"] = stochrsi(df["close"], window=14, smooth1=3, smooth2=3)
        k_vals = df["stoch_rsi_k"].dropna().tolist()
        if len(k_vals) >= 4:
            slope = k_vals[-1] - k_vals[-4]
            score = max(0.0, min(round(slope * 5, 4), 1.0))
            return score, round(slope, 6)
    except Exception as e:
        logging.warning(f"[WARN] Failed to compute stoch slope for {symbol}: {e}")
    return 0.0, None


def load_kline_volumes(symbol):
    today = datetime.utcnow().strftime("%Y-%m-%d")
    filepath = SNAPSHOT_BASE / today / f"{symbol.upper()}_15m_klines.json"
    if not filepath.exists():
        return None, None
    try:
        with open(filepath, "r") as f:
            klines = json.load(f)
        volumes = [float(c[5]) for c in klines[-9:]]
        if len(volumes) < 2:
            return None, None
        return volumes[-1], sum(volumes[:-1]) / (len(volumes) - 1)
    except:
        return None, None


def compute_subscores(symbol, side: str = "long"):
    data = r.get(f"{symbol.upper()}_1h") if r else None
    data = json.loads(data) if data else {}

    price = extract_float(data.get("latest_close"))
    ema50 = extract_float(data.get("EMA50"))
    adx = extract_float(data.get("ADX14"))
    atr = extract_float(data.get("ATR"))
    macd = extract_float(data.get("MACD"))
    macd_signal = extract_float(data.get("MACD_signal"))
    macd_hist = extract_float(data.get("MACD_Histogram"))
    macd_hist_prev = extract_float(data.get("MACD_Histogram_Prev"))

    rsi = extract_float(r.get(f"{symbol.upper()}_15m_RSI14")) if r else 0.0
    k = extract_float(r.get(f"{symbol.upper()}_15m_StochRSI_K")) if r else 0.0
    d = extract_float(r.get(f"{symbol.upper()}_15m_StochRSI_D")) if r else 0.0

    current_vol = extract_float(r.get(f"{symbol.upper()}_15m_volume")) if r else 0.0
    sma9_vol = extract_float(r.get(f"{symbol.upper()}_15m_volume_sma9")) if r else 0.0
    if current_vol == 0 or sma9_vol == 0:
        current_vol, sma9_vol = load_kline_volumes(symbol)

    stoch_slope_score, stoch_raw_slope = compute_stoch_slope(symbol)

    # Directional gates (side-aware)
    if side == "long":
        ema_trend_ok = price > ema50
        macd_histogram_score = 1.0 if (macd > macd_signal and macd_hist > macd_hist_prev) else 0.0
        macd_cross_score = 1.0 if macd > macd_signal else 0.0
        rsi_direction_score = min(max((rsi - 30) / 20, 0), 1)
        stoch_cross_score = min(max((k - d) / 25, 0), 1) if (k > d and k < 80) else 0.0
        stoch_extreme_penalty = 0.0 if (k > 90 and d > 90) else 1.0
        # Mean reversion penalty when far above EMA
        mean_reversion_score = 1.0
        if price > ema50 and atr > 0:
            dist = (price - ema50) / atr
            if dist > 3:
                mean_reversion_score = 0.0
            elif dist > 2:
                mean_reversion_score = 0.25
            elif dist > 1.5:
                mean_reversion_score = 0.5
    else:  # short
        ema_trend_ok = price < ema50
        macd_histogram_score = 1.0 if (macd < macd_signal and macd_hist < macd_hist_prev) else 0.0
        macd_cross_score = 1.0 if macd < macd_signal else 0.0
        rsi_direction_score = min(max((70 - rsi) / 20, 0), 1)
        stoch_cross_score = min(max((d - k) / 25, 0), 1) if (d > k and k > 20) else 0.0
        stoch_extreme_penalty = 0.0 if (k < 10 and d < 10) else 1.0
        # Mean reversion penalty when far below EMA
        mean_reversion_score = 1.0
        if price < ema50 and atr > 0:
            dist = (ema50 - price) / atr
            if dist > 3:
                mean_reversion_score = 0.0
            elif dist > 2:
                mean_reversion_score = 0.25
            elif dist > 1.5:
                mean_reversion_score = 0.5

    subscores = {
        "macd_histogram": macd_histogram_score,
        "macd_trend_cross": macd_cross_score,
        "rsi_direction": rsi_direction_score,
        "stoch_rsi_cross": stoch_cross_score,
        "stoch_extreme_penalty": stoch_extreme_penalty,
        "adx_rising": (min(adx / 20, 1.0) if adx > 10 else 0.0),
        "ema_trend_ok": (1.0 if ema_trend_ok else 0.0),
        "mean_reversion_score": mean_reversion_score,
        "volume_penalty": (0.0 if (sma9_vol and current_vol and current_vol > sma9_vol * 2) else 1.0) if OPTIONS.get("use_volume_penalty", True) else 0.0,
        "stoch_rsi_slope": stoch_slope_score,
    }

    base_score = sum(subscores[k] * WEIGHTS.get(k, 0) for k in subscores)
    mult = btc_sentiment_multiplier(side)
    adjusted = round(base_score * mult, 4)

    raw_indicators = {
        "price": price,
        "ema50": ema50,
        "adx": adx,
        "atr": atr,
        "macd": macd,
        "macd_signal": macd_signal,
        "macd_hist": macd_hist,
        "macd_hist_prev": macd_hist_prev,
        "rsi": rsi,
        "stoch_rsi_k": k,
        "stoch_rsi_d": d,
        "volume": current_vol,
        "volume_sma9": sma9_vol,
        "stoch_slope": stoch_raw_slope,
    }

    return adjusted, subscores, mult, raw_indicators


def write_to_history_log(entry, date_str):
    path = FORK_HISTORY_BASE / date_str / "fork_scores.jsonl"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "a") as f:
        f.write(json.dumps(entry) + "\n")


def main():
    global REDIS_PREFIX, r, MIN_SCORE, WEIGHTS, OPTIONS
    parser = argparse.ArgumentParser(description="Fork score filter for leverage (long/short)")
    parser.add_argument("--side", choices=["long", "short"], default="long")
    parser.add_argument("--config", type=str, default=str(DEFAULT_CONFIG_PATH))
    parser.add_argument("--output", type=str, default=str(DEFAULT_OUTPUT_FILE))
    parser.add_argument("--backtest", type=str, default=str(DEFAULT_BACKTEST_CANDIDATES_FILE))
    parser.add_argument("--redis-prefix", type=str, default=REDIS_PREFIX)
    parser.add_argument("--no-redis", action="store_true", help="Run without connecting to Redis")
    parser.add_argument("--redis-host", type=str, default="localhost")
    parser.add_argument("--redis-port", type=int, default=6379)
    parser.add_argument("--input", type=str, help="Path to JSON list of symbols (e.g., lev/data/perps/lev_symbols.json)")
    args = parser.parse_args()

    REDIS_PREFIX = args.redis_prefix or "LEV"

    if not args.no_redis:
        try:
            r = redis.Redis(host=args.redis_host, port=args.redis_port, decode_responses=True)
            # quick ping to confirm availability (optional, ignore failures)
            try:
                r.get("__ping__")
            except Exception:
                pass
        except Exception as _e:
            logging.warning(f"Redis unavailable, continuing with DummyRedis: {_e}")
            r = DummyRedis()
    else:
        logging.info("Running with --no-redis: using DummyRedis (no external state)")
        r = None

    MIN_SCORE, WEIGHTS, OPTIONS, full_cfg = load_config(Path(args.config))

    # Override output paths; append side so long/short don‚Äôt collide
    output_path = Path(args.output)
    if output_path.suffix == "":
        output_path = output_path / f"final_fork_rrr_trades_{args.side}.json"
    elif output_path.name == Path(DEFAULT_OUTPUT_FILE).name:
        output_path = output_path.with_name(f"final_fork_rrr_trades_{args.side}.json")

    backtest_path = Path(args.backtest)
    if backtest_path.suffix == "":
        backtest_path = backtest_path / f"fork_backtest_candidates_{args.side}.json"
    elif backtest_path.name == Path(DEFAULT_BACKTEST_CANDIDATES_FILE).name:
        backtest_path = backtest_path.with_name(f"fork_backtest_candidates_{args.side}.json")

    # Load symbols (prefer --input), then restrict to leverage universe
    if args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            logging.error(f"Missing --input file: {input_path}")
            return
        with open(input_path) as f:
            raw_symbols = json.load(f)
    else:
        if not FORK_INPUT_FILE.exists():
            logging.error(f"Missing: {FORK_INPUT_FILE}")
            return
        with open(FORK_INPUT_FILE) as f:
            raw_symbols = json.load(f)

    # Normalize and filter to allowed 10-coin universe
    universe_pairs = { _to_usdt_symbol(t) for t in ALLOWED_TICKERS }
    symbols = []
    for s in raw_symbols:
        pair = _to_usdt_symbol(str(s))
        if pair in universe_pairs:
            symbols.append(pair.replace("USDT", ""))  # keep bare ticker for downstream lookups
    symbols = sorted(list(dict.fromkeys(symbols)))

    if not symbols:
        logging.warning("No symbols after filtering to leverage universe; nothing to score.")
        with open(output_path, "w") as f:
            json.dump([], f)
        with open(backtest_path, "w") as f:
            json.dump([], f)
        logging.info(f"üìÇ Saved 0 {args.side} trades to {output_path}")
        logging.info(f"üìä Backtest candidates saved to {backtest_path}")
        return

    logging.info(f"Evaluating {len(symbols)} symbols: {', '.join(symbols)}")

    # Reset namespaced Redis keys
    if r:
        r.delete(REDIS_SET(args.side))
        r.delete(REDIS_FINAL_TRADES(args.side))

    now = datetime.utcnow()
    now_ts = int(now.timestamp() * 1000)
    ts_iso = now.isoformat() + "Z"
    today = now.strftime("%Y-%m-%d")
    results, candidates = [], []

    for sym in symbols:
        score, subs, mult, raw_indicators = compute_subscores(sym, side=args.side)
        price = raw_indicators.get("price", 0.0)
        passed = score >= MIN_SCORE and (
            subs.get("rsi_direction", 0) > 0 or subs.get("stoch_rsi_cross", 0) > 0
        )

        log = {
            "symbol": sym.upper(),
            "score": score,
            "timestamp": now_ts,
            "ts_iso": ts_iso,
            "score_hash": "_".join([f"{k}:{subs[k]}" for k in subs]),
            "score_components": subs,
            "btc_multiplier": mult,
            "entry_price": price, #"entry_price": extract_float(r.get(f"{sym.upper()}_1h_latest_close")),
            "raw_indicators": raw_indicators,
            "passed": passed,
            "source": "fork_score_filter",
        }

        write_to_history_log(log, today)
        candidates.append(
            {
                "symbol": log["symbol"],
                "score": log["score"],
                "timestamp": log["timestamp"],
                "ts_iso": log["ts_iso"],
                "raw_indicators": log["raw_indicators"],
            }
        )

        if passed and r:
            trade = {
                "symbol": sym.upper(),
                "pair": f"{sym.upper()}_USDT",
                "score": score,
                "meta": subs,
                "score_hash": log["score_hash"],
                "timestamp": now_ts,
                "ts_iso": ts_iso,
                "side": args.side,
            }
            r.sadd(REDIS_SET(args.side), sym)
            r.rpush(REDIS_FINAL_TRADES(args.side), json.dumps(trade))
            results.append(trade)
        elif passed and not r:
            trade = {
                "symbol": sym.upper(),
                "pair": f"{sym.upper()}_USDT",
                "score": score,
                "meta": subs,
                "score_hash": log["score_hash"],
                "timestamp": now_ts,
                "ts_iso": ts_iso,
                "side": args.side,
            }
            results.append(trade)

        verdict = "‚úÖ" if passed else "‚ùå"
        log_lines = [
            f"{verdict} {sym.upper()} | Score: {score:.3f} | Mult: {mult:.2f}"
        ] + [
            f"    ‚Ä¢ {k.replace('_',' ').title():25}: {'‚úÖ' if subs[k] >= 1 else '‚ùå' if subs[k] == 0 else f'{subs[k]:.3f}'}"
            for k in subs
        ]
        logging.info("\n" + "\n".join(log_lines))

    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    with open(backtest_path, "w") as f:
        json.dump(candidates, f, indent=2)

    logging.info(f"üìÇ Saved {len(results)} {args.side} trades to {output_path}")
    logging.info(f"üìä Backtest candidates saved to {backtest_path}")


if __name__ == "__main__":
    main()
