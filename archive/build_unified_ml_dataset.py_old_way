#!/usr/bin/env python3
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from dateutil import parser as dtparser

# === Paths ===
PROJECT_ROOT = Path("/home/signal/market7").resolve()
DCA_LOG_PATH = PROJECT_ROOT / "dca/logs"
ENRICHED_PATH = PROJECT_ROOT / "ml/enriched"
SNAPSHOT_PATH = PROJECT_ROOT / "ml/datasets/recovery_snapshots"
OUTPUT_PATH = PROJECT_ROOT / "ml/datasets/unified"

# === Helpers ===
def load_jsonl(path: Path):
    if not path.exists():
        return []
    with open(path, "r") as f:
        return [json.loads(line) for line in f if line.strip()]

def extract_snapshot_meta(snapshots):
    times, dd_vals, score_vals, rsi_vals = [], [], [], []
    for snap in snapshots:
        ts = snap.get("timestamp")
        try:
            dt = dtparser.parse(ts)
        except:
            continue
        dd = snap.get("drawdown_pct")
        sc = snap.get("current_score")
        rsi = snap.get("rsi")
        if dd is None or sc is None or rsi is None:
            continue
        times.append(dt)
        dd_vals.append(dd)
        score_vals.append(sc)
        rsi_vals.append(rsi)

    if not times:
        return {}

    score_trend = score_vals[-1] - score_vals[0]
    rsi_trend = rsi_vals[-1] - rsi_vals[0]
    max_dd = max(dd_vals)
    min_score = min(score_vals)
    min_rsi = min(rsi_vals)
    try:
        idx_max = dd_vals.index(max_dd)
        time_to_max = (times[idx_max] - times[0]).total_seconds() / 60.0
    except:
        time_to_max = 0.0

    return {
        "snapshot_score_trend": score_trend,
        "snapshot_rsi_trend": rsi_trend,
        "snapshot_max_drawdown": max_dd,
        "snapshot_min_score": min_score,
        "snapshot_min_rsi": min_rsi,
        "snapshot_time_to_max_drawdown_min": time_to_max
    }

# === Main Builder ===
def build_dataset(date_str):
    enriched_file = ENRICHED_PATH / date_str / "enriched_data.jsonl"
    dca_file = DCA_LOG_PATH / date_str / "dca_log.jsonl"
    output_file = OUTPUT_PATH / f"{date_str}.jsonl"
    output_file.parent.mkdir(parents=True, exist_ok=True)

    enriched = load_jsonl(enriched_file)
    dca_log = load_jsonl(dca_file)

    enriched_map = {e["trade"]["trade_id"]: e for e in enriched if "trade" in e}
    output = []
    skipped = 0

    for dca in dca_log:
        if dca.get("decision") != "fired":
            continue

        deal_id = dca.get("deal_id")
        short = dca.get("symbol", "").replace("USDT_", "")
        trade_id = int(deal_id) if deal_id else None
        enriched_entry = enriched_map.get(trade_id)

        if not enriched_entry:
            skipped += 1
            continue

        snapshot_file = SNAPSHOT_PATH / f"{short}_{deal_id}.jsonl"
        snapshots = load_jsonl(snapshot_file)
        meta = extract_snapshot_meta(snapshots) if snapshots else {}

        btc = enriched_entry.get("btc_context", {}).get("entry", {})
        btc_status = dca.get("btc_status", "UNKNOWN")
        row = {
            "deal_id": deal_id,
            "step": dca.get("step"),
            "entry_score": dca.get("entry_score"),
            "current_score": dca.get("current_score"),
            "tp1_shift": dca.get("tp1_shift"),
            "safu_score": dca.get("safu_score"),
            "rsi": dca["indicators"].get("rsi"),
            "macd_histogram": dca["indicators"].get("macd_histogram"),
            "adx": dca["indicators"].get("adx"),
            "macd_lift": dca["indicators"].get("macd_lift"),
            "rsi_slope": dca["indicators"].get("rsi_slope"),
            "drawdown_pct": dca["indicators"].get("drawdown_pct"),
            "recovery_odds": dca.get("recovery_odds"),
            "confidence_score": dca.get("confidence_score"),
            "zombie_tagged": dca.get("zombie_tagged", False),
            "btc_rsi": btc.get("rsi"),
            "btc_macd_histogram": btc.get("macd_histogram"),
            "btc_adx": btc.get("adx"),
            "btc_status": btc_status,
            "volume_sent": dca.get("volume_sent"),
            "recovery_label": 1 if enriched_entry.get("status") == "passed" else 0,
            "safu_good_but_zombie": enriched_entry.get("safu_good_but_zombie", False),
            **meta
        }
        output.append(row)

    with open(output_file, "w") as f:
        for row in output:
            f.write(json.dumps(row) + "\n")

    print(f"✅ Saved {len(output)} rows to {output_file}")
    print(f"⚠️ Skipped {skipped} unmatched DCA entries")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--date", required=True, help="Date in YYYY-MM-DD format")
    args = parser.parse_args()
    build_dataset(args.date)
